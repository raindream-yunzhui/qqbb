- ## [[Papars]]
	- #neuroscience-inspired #multi-modal[Nature Machine Intelligence丨人类大脑中的高级视觉表征与大型语言模型相一致](https://mp.weixin.qq.com/s/U6TZM4JrHIpd86TcnLsIuQ)
	  collapsed:: true
		- 探讨了一个核心问题：**我们人类大脑理解视觉场景的高级方式，是否和大型语言模型理解语言的方式是相通的？**
		- ### 一、核心思想：一个大胆的猜想
			- 我们可以先把大脑和计算机模型分开想：
				- **大脑**：
					- 当你看到一张“一只狗站在船上”的照片时，你的眼睛接收像素信息，但你的大脑理解的远不止是“狗”和“船”这两个物体。它几乎瞬间就理解了整个**场景**：这可能是在一个湖上，狗看起来很开心，也许在划船等等。这种对场景的、包含关系和上下文的理解，就是所谓的“高级视觉表征”。
				- **大型语言模型**：
					- 像ChatGPT这样的LLM，它们虽然不“看”图，但通过阅读海量文本，学会了语言中的**世界知识**和**上下文关系**。比如，它知道“狗”和“船”这两个词经常在什么样的句子中共现，能理解“一只狗站在船上”这句话所描绘的大致场景。
			- 这篇论文提出了一个大胆的猜想：**大脑经过层层处理，最终形成的对视觉场景的“理解”（高级表征），其形态可能非常接近于LLM对描述该场景的句子所产生的“理解”（嵌入向量）。**
			- 简单来说，尽管一个是处理视觉，一个是处理语言，但它们最终可能把信息“翻译”成了同一种“格式”或“语言”。
			- > **举个例子**：
			  > 想象一个只会说中文的人（大脑）和一个只会说英文的人（LLM）。他们一个看画，一个读诗。突然有一天，我们发现，当他们看到/读到“夕阳下的孤独旅人”这个意境时，他们内心涌起的情感波动（表征）的**模式**竟然是一样的！这篇论文就在做类似的发现。
		- ### 二、他们是怎么验证的？（研究方法）
			- **1. 数据基础：大脑扫描 + 图片描述**
				- 他们使用了一个叫“自然场景数据集”的庞大fMRI数据库。让参与者躺在7T超高场强的磁共振仪器里观看成千上万张真实的照片（比如来自COCO数据集的照片），同时记录他们大脑的活动。关键是，每张照片都有好几个人工写的文字描述（例如：“一只狗站在船上”）。
			- **2. 核心方法一：从“文字”预测“大脑”**
				- **步骤**：他们将照片的**文字描述**（注意，不是图片本身！）输入到LLM（文中主要用的是MPNet模型），得到一个代表这句话含义的数学向量（称为“嵌入”）。
				- **分析**：然后，他们用这个LLM向量去预测参与者看到对应图片时的大脑活动。
				- **结果**：他们发现，==LLM向量能够相当准确地预测出大脑中高级视觉区域==（比如负责识别物体、场景、人脸的区域）的活动。更神奇的是，他们甚至能用这个模型“虚构”出大脑对特定概念的反应，比如“人”和“风景”的对比，结果模型预测出的大脑活跃区域，和神经科学已知的“人脸识别区”、“场景识别区”完全吻合。
			- **3. 核心方法二：从“大脑”反推“文字”**
				- *   **步骤**：他们训练了一个简单的模型，这次是反过来，从**大脑活动**去预测**LLM向量**。
				  *   **分析**：得到预测的LLM向量后，他们在一个包含310万句描述的“字典”里，寻找最接近的句子。
				  *   **结果**：他们成功地从大脑活动中“解码”出了非常接近原始图片的描述。例如，参与者看到的图片描述是“两只长颈鹿站在树林旁”，而解码出的句子是“长颈鹿在灌木丛中彼此靠近站着”。这说明，大脑中的信息确实可以用LLM的“语言”来解读和表达。
			- **4. 为什么是LLM？关键的控制实验**
				- 有人可能会问：能预测大脑活动，是不是只是因为LLM编码了图片中的“物体”信息（狗、船）？为了回答这个问题，他们做了对比：
					- **对照组1**：只用物体类别列表（如 [狗，船]）的模型。
					- **对照组2**：只用名词或动词的LLM向量。
					- **实验组**：用完整句子描述的LLM向量。
				- **结论**：
					- 完整句子的LLM向量预测大脑活动的能力**远胜于**所有对照组。这说明，==LLM的强大之处在于它能**整合整个句子的复杂信息**（不仅仅是单个词，还有词与词之间的关系和上下文），而这部分“超越物体”的整合信息，恰恰是大脑高级视觉表征的关键。==
		- ### 三、更进一步的发现：训练“看得懂”图的AI
			- 如果大脑真的在做“从图到LLM向量”的转换，那么，如果我们训练一个AI也来做这件事，它学到的中间表征会不会更像大脑？
			- **他们真的这么做了：**
				- 他们训练了一种叫“循环卷积神经网络”的AI模型，它的任务很简单：**输入一张图片，输出这张图片对应的描述文字的LLM向量**。
			- **惊人的结果出现了：**
				- 这个被训练来理解图片和LLM向量之间关系的AI，它内部的表征（可以理解为它的“思维模式”）与人类大脑活动的相似度，**甚至超过了它要模仿的目标——LLM向量本身**。这意味着它可能学到了一些视觉特有的、但同样对理解场景有用的信息。
				- 在与其他多种顶尖AI模型的对比中，这个用LLM作为“老师”训练出来的模型，在预测大脑活动方面**取得了最好的成绩**。而且，它达成这一成就所使用的训练图片数量，比其他模型少了几个数量级（只用了几万张图，而非数百万甚至上亿张）。
				- 这说明，==**“学会用LLM的方式描述世界”是一个极其高效且与大脑处理方式高度一致的学习目标。**==
		- ### 四、总结与意义
			- **语言与视觉的桥梁**：
				- 人类大脑处理视觉信息的高级表征，与大型语言模型处理语言信息的表征，存在着深刻的相似性。这意味着语言和视觉在大脑深处可能共享某种通用的“语义代码”。
			- **信息整合是关键**：
				- 大脑（以及好的AI模型）不是简单地识别物体，而是**理解物体之间的关系、场景的上下文和含义**。LLM正是通过阅读海量文本，学会了这种整合复杂信息的能力。
			- **新的AI训练范式**：
				- 使用LLM的嵌入向量作为训练目标，可以引导AI模型学习到更接近人类大脑的视觉表征，而且效率更高。这为构建更智能、更“类脑”的人工智能提供了新的方向。
		- ### 通俗的比喻
			- 想象大脑是一个顶级厨师，他能尝一道菜（视觉输入），然后精准地写出它的食谱（高级理解）。
			- LLM是一个美食评论家，他读过世界上所有的菜谱和食评（文本训练），所以当你给他一份食谱时，他能深刻理解这道菜的精髓。
			- 这篇论文发现，厨师尝菜后脑中形成的“感觉”，和美食评论家读到菜谱后脑中形成的“感觉”，惊人地相似。更厉害的是，如果我们让一个学徒（AI模型）看着菜的照片，去学习预测美食评论家会怎么写这道菜的食评，那么这个学徒最终会成为一个非常理解厨师思维的优秀厨师。
			- 这项研究打破了视觉和语言的界限，为我们理解智能的本质——无论是生物的还是人工的——打开了一扇新的大门。
	- #neuroscience [大脑自带“计时器”！最新Nature子刊揭示：即便“发呆”“躺平”，你的海马体也在悄悄记录时间的流逝](https://mp.weixin.qq.com/s/16GHbhYhC7g7VsfCTkDHJw) 感觉这篇文章完全sb
	  collapsed:: true
		- 这篇论文研究的是一个非常有趣的问题：**我们的大脑在什么都不做的时候，是不是也在默默地“记录”时间的流逝？**
		- ### 一、核心问题：大脑里有个“内置时钟”吗？
			- 想象一下，你不需要看钟表，也能大概感觉到过了多长时间。这篇论文就是想探索，我们的大脑是否天生就有一个“内置的时间追踪器”。
			- **以前的研究发现**：当我们做任务时（比如看图片、听故事），大脑中一个叫 **“海马体”** 的区域和它旁边的 **“内嗅皮层”** 的活动模式会随着时间变化。时间隔得越久，它们活动的“样子”就越不像。
			- **好比**：你周一和周五分别拍了两张自拍照，虽然都是你，但发型、表情、光线可能都有些细微差别。大脑处理信息也是这样，不同时间点，其活动模式会略有不同。
			- **这篇论文想问的新问题**：那么，在我们**什么都不想、什么都不做**的时候（比如发呆、休息），海马体和内嗅皮层的活动会不会也随着时间自然地发生变化，从而“记录”下时间的流逝呢？
		- ### 二、研究方法：给两个人的大脑“打卡”30天
			- 为了回答这个问题，研究人员采用了一种非常精细的方法：
			- **密集采样**：他们找了两位志愿者（一男一女），在连续30天里，**每天都**请他们到实验室进行一次“静息态”功能性磁共振成像扫描。
				- “静息态”就是让参与者躺在扫描仪里，保持清醒但什么都不用做，让大脑自由活动。
				- 这就像每天在固定时间给大脑拍一张“静态工作快照”。
			- **测量什么**：研究人员关注的不是大脑某个单独区域的活动强弱，而是**大脑内部的“连接模式”**。
			    *   **什么是连接模式？** 你可以把大脑的不同区域想象成一群朋友。连接模式就是看，在休息状态下，海马体或内嗅皮层这位“核心朋友”和全脑其他所有“朋友”之间的互动默契程度（即信号同步性）。每天的快照就是一张“全脑交友关系图”。
			- **如何分析**：他们比较了任意两天（比如第1天和第5天，第10天和第25天）的“交友关系图”的相似度，并看这个相似度和这两天之间相隔的时间有什么关系。
		- ### 三、主要发现：大脑确实在默默地“数着日子”
			- 分析之后，他们得到了几个非常清晰的结论：
			- **时间越久，模式越不像**：
				- 两个人在休息状态下，海马体和内嗅皮层与全脑的“连接模式”，其相似度**确实会随着间隔时间的增加而显著下降**。
				- **好比**：比较你第1天和第2天的“交友关系图”，会发现非常像；但比较第1天和第30天的图，就会发现差别很大。这说明，大脑的连接模式在自发地、缓慢地“漂移”，而这个漂移与时间的流逝紧密相关。
			- **大脑的“前后分工”**：
				- **海马体像一个香蕉，有前（粗）后（细）之分**。研究发现，**前部海马体**的连接模式随时间的变化比**后部海马体**更明显。
				- **内嗅皮层也分内外**：**外侧部分**的连接模式随时间变化很明显，而**内侧部分**则不太明显。
				- 这说明，大脑对时间的记录功能，主要集中在前部海马体和外侧内嗅皮层，存在着一种 **“功能梯度”**。
			- **排除了其他干扰**：
				- 你可能会想，是不是因为每天心情好坏、身体激素变化或者头动了一下导致的？
				- 研究人员严格控制了这些因素（每天都会测量情绪、激素水平和头动情况），发现即使在排除这些影响后，**“时间”本身仍然是导致大脑连接模式变化的最强因素**。
			- **和谁“交友”变化最大？**
				- 研究发现，内嗅皮层和海马体与某些特定的“朋友圈”（即大脑网络）的连接变化，对时间最敏感。
				- 主要是**默认模式网络**和**视觉网络**等。默认模式网络在我们发呆、回忆过去时特别活跃；视觉网络处理视觉信息。它们与记忆的形成密切相关。
		- ### 四、打个总结与比喻
			- 你可以把这项研究想象成这样：
				- **你的大脑（特别是海马体和内嗅皮层）就像一个每天都在自动拍照的相机。**它拍的不是景物，而是它自己内部亿万神经元此刻的“协作关系图”。
				- **每一天，这张关系图都会有极其微小的、自发的改变。**
				  *   日积月累，**昨天和前天的图还很相似，但上个月和这个月的图看起来就大不相同了**。
				  *   大脑不需要你主动去做什么，它就在休息状态下，通过这种缓慢的“模式漂移”，**无意识地为每一天都盖下了一个独特的“时间戳”**。
		- ### 五、这项研究的意义与未来
			- **科学意义**：它首次直接证明了，即使在完全休息、无任务的状态下，人脑也内在地嵌入了时间流逝的神经信号。这为我们理解“时间感”的神经基础打开了新的大门。
			- **潜在应用**：这种内在的“时间戳”很可能为我们的**情景记忆**（即对个人经历的记忆）服务。它可能帮助我们将事件与发生的时间绑定在一起，从而回答“那件事是什么时候发生的？”这样的问题。
			- **局限与未来**：这项研究的结论基于对两个人的深入观察，虽然非常精细，但需要在更大的人群中进行验证。未来还需要研究这种休息状态下的时间信号，如何与我们在完成任务时的时间感知和记忆相互影响。
			-
	- #vla #world-model [阿里新研究：统一了VLA和世界模型](https://mp.weixin.qq.com/s/IW6H33317ePE4PRI3lnCPA) 感觉思想和方法都很简单直接
	  collapsed:: true
		- **《WorldVLA：迈向自回归动作世界模型》** 的论文。我会尽量用通俗易懂的语言，配合生活中的例子，帮助你理解这个研究的内容和意义。
		- ### 一、这篇论文想解决什么问题？
		  collapsed:: true
			- 想象一下你在教一个机器人拿水杯。现在的机器人通常有两种“思考”方式：
				- **动作模型**：看到水杯后，直接决定“伸手去抓”。
				  **世界模型**：看到水杯和你伸手的动作，预测“水杯会被拿起来”。
			- **但问题是**：
				- 动作模型只输出动作，不理解动作会导致什么后果。
				- 世界模型只预测画面，不能直接控制机器人行动。
			- 这篇论文提出的 **WorldVLA**，就是要**把这两种能力合二为一**：让机器人既能理解动作、生成动作，又能预测动作带来的画面变化。
		- ### 二、WorldVLA 是什么？
		  collapsed:: true
			- WorldVLA 是一个 **“能看、能想、能动”的统一模型**。你可以把它想象成一个**既会画画又会做动作的智能大脑**。
			- 它由两部分组成：
				- **动作模型**：根据看到的画面和指令，决定下一步做什么动作。
					- 例如：看到“水杯在桌上”，指令是“拿水杯”，它就输出“伸手”这个动作。
				- **世界模型**：根据当前画面和动作，预测下一个画面会是什么。
					- 例如：看到“手伸向水杯”，它就预测“水杯被拿起”的画面。
				- 这两个模型**共用同一个“大脑”**（一个大语言模型），所以它们可以互相帮助、互相学习。
		- ### 三、他们遇到了什么问题？怎么解决的？
		  collapsed:: true
			- **问题：动作链条越长，错误越多**
				- 当机器人需要连续做多个动作时（比如“走近水杯 → 伸手 → 抓握”），如果第一步就错了，后面的动作也会跟着错。这就像你蒙着眼睛走路，第一步走歪了，后面越走越歪。
			- 解决方案：**注意力掩码**
				- 他们设计了一种“选择性屏蔽”机制：
					- 在生成每一个动作时，**不看之前已经生成的动作**，只看最初的画面和指令。
					- 这样就避免了错误传递，每个动作都基于最原始的信息独立生成。
				- 👉 **好比学跳舞时，不看前面同学的动作，只看老师的示范，这样就不会被带偏。**
		- ### 四、他们做了什么实验？结果如何？
		  collapsed:: true
			- 他们在 **LIBERO** 机器人任务平台上测试，包括：
				- 空间任务（如把碗放在某个位置）
				- 物体任务（如拿起特定物体）
				- 目标任务（如完成多步任务）
				- 长任务（如连续执行10个动作）
			- **实验结果：**
				- **WorldVLA 比单一模型更强**：
					- 动作成功率比单独的动作模型高 **4%**
					- 预测的画面质量比世界模型更好（FVD 指标下降 **10%**）
				- **动作模型和世界模型互相帮助**：
					- 世界模型让机器人更懂“物理规律”，动作更准确。
					- 动作模型让世界模型更懂“行为意图”，画面预测更合理。
				- **注意力掩码有效**：
					- 使用掩码后，连续动作的成功率提升了 **4% ~ 23%**，尤其是在长任务中效果显著。
		- ### 五、这篇论文的意义是什么？
		  collapsed:: true
			- **统一模型**：第一次把“动作理解”和“画面预测”放在同一个模型中训练。
			- **解决错误累积**：提出“注意力掩码”，让机器人在长序列任务中表现更稳定。
			- **推动机器人学习**：为未来能“自主思考”的机器人打下了基础。
		- ### 六、举个例子来理解整个过程
		  collapsed:: true
			- 假设任务是 **“把奶酪放进盘子”**：
			- 1. **动作模型**：看到奶酪和盘子 → 决定“抓奶酪”
			  2. **世界模型**：看到“手抓奶酪” → 预测“奶酪被拿起”
			  3. **动作模型**：看到“奶酪被拿起” → 决定“移动向盘子”
			  4. **世界模型**：看到“手移动向盘子” → 预测“奶酪被放入盘子”
			- 如果某一步错了（比如没抓住奶酪），世界模型会预测出“失败画面”，动作模型就能及时调整策略，而不是一错再错。
		- ### 总结
		  collapsed:: true
			- WorldVLA 是一个**既能理解动作、又能预测画面的机器人大脑**，它通过“注意力掩码”避免错误累积，在多项任务中表现优于单一模型。这项研究让我们离“能自主思考和行动”的机器人更近了一步。
	- #neuroscience [最新Nature正刊 | 反直觉！感觉预期为何“绕开”感觉区，直达运动皮层？](https://mp.weixin.qq.com/s/QoWaUHG8NyfOYkeGGgYtHQ)
	  collapsed:: true
		- 《感觉预期塑造运动神经回路中的群体神经动力学》
		- ### 🧠 **一、研究背景：我们为什么要研究“运动准备”？**
		  collapsed:: true
			- 想象一下你要去拿桌上的水杯。在你真正动手之前，你的大脑其实已经“悄悄地”开始准备了——它已经在计划要用多大的力气、手要往哪个方向移动。这个“准备阶段”对你能否准确拿到水杯至关重要。
			- 过去的研究大多关注的是 **“我们自己发起的动作”**（比如主动去拿杯子）是如何在大脑中准备的。但生活中，我们的动作常常是被 **外界干扰** 触发的——比如你端着咖啡时被人撞了一下，你得马上稳住杯子。这种时候，你的大脑能不能 **提前准备** 来应对这种干扰呢？
			- 这篇论文的核心思想就是：
			  > **==我们不仅能准备“自己要做的动作”，也能准备“应对即将到来的感觉干扰”。==**
		- ### 🎯 **二、他们做了什么实验？**
		  collapsed:: true
			- 研究者设计了一个巧妙的实验：
			- #### 👉 对人类和猴子的实验：
				- 参与者坐在一个机器人装置（KINARM）里，手臂被固定，面前有屏幕。
				- 他们需要把手保持在一个小目标区域内。
				- 屏幕上会显示一个 **概率提示**（比如一个箭头），告诉他们接下来手臂 **很可能会被往哪个方向推**（屈肘或伸肘），以及这个方向的可能性有多大（25%、50%、75%、100%）。
				- 稍后，机器人确实会推一下他们的手肘，他们需要尽快把手移回目标位置。
				  
				  > ✅ **关键点**：参与者会根据提示的“可能性”来调整他们的反应——如果提示说“很可能会被往前推”，他们就会提前准备好抵抗向前的力。
		- ### 📊 **三、他们发现了什么？**
		  collapsed:: true
			- #### 1. **行为上：预期让反应更精准**
				- 当提示越确定（例如100%），参与者的纠正动作就越快、越准确。
				- 这说明他们确实利用了“感觉预期”来优化运动反应。
			- #### 2. **生理上：预期影响“长潜伏期反射”**
				- 我们的肌肉在被拉长时会有两种反射：
					- **短潜伏期反射（20–50毫秒）**：由脊髓控制，不受意识影响。
					- **长潜伏期反射（50–100毫秒）**：涉及大脑皮层（尤其是运动皮层），可以被认知因素调节。
				- 研究者发现：**只有长潜伏期反射** 会受到概率提示的影响 → 说明大脑皮层参与了这种“预期准备”。
			- #### 3. **神经记录上：预期信息广泛存在于运动回路中**
				- 他们在猴子脑中多个区域植入电极记录神经元活动，包括：
					- **初级运动皮层（M1）**
					- **背侧前运动皮层（PMd）**
					- **前额叶皮层（dlPFC）**
					- **初级体感皮层（S1）**
					- **丘脑等 subcortical 区域**
				- 使用一种叫 **dPCA** 的分析方法，他们发现：
					- **预期信息（概率）** 在 PMd 和 M1 中最强，而在 S1 中很弱。
					- 这些信息以 **简单的几何方式** 编码：神经活动的模式直接对应“可能性大小”。
					- 大脑中还有一个 **条件无关的信号**，像一个“警报”，一有扰动就触发反应，不管方向如何。
		- ### 🤖 **四、他们还用计算机模型验证了结论**
		  collapsed:: true
			- 为了理解大脑为什么这样工作，研究者训练了一个 **神经网络模型** 来控制一个虚拟手臂：
				- 模型也接受“概率提示”，并学会根据提示调整反应。
				- 当他们在模型中 **“关闭”负责概率信息的神经维度** 时，模型就不再能根据预期优化反应。
			- 他们还发现：**只有当模型能快速检测到“有扰动发生”（即使不知道方向），预期才有用**。
			- > 💡 这说明：大脑之所以能利用预期，是因为它能 **快速检测到“有事发生”**，然后在明确“是什么事”之前，先根据预期做出初步反应。
		- ### 🧩 **五、总结与意义**
		  collapsed:: true
			- | 研究发现 | 通俗解释 |
			  |----------|----------|
			  | 我们能用“感觉预期”优化运动反应 | 就像开车时预判前方有车，你会提前轻踩刹车 |
			  | 预期信息广泛存在于运动脑区 | 不只是“感觉区”在预测，而是整个“运动系统”都在准备 |
			  | 神经编码方式简单直接 | 大脑用“比例”来编码可能性，比如75%就放在50%和100%之间 |
			  | 模型证实：快速检测是关键 | 你必须先知道“有事情发生”，才能用上预期 |
		- ### 🌍 **六、这对我们有什么意义？**
		  collapsed:: true
			- 这项研究不仅深化了我们对“大脑如何准备动作”的理解，也有潜在的应用价值：
				- **康复医学**：帮助中风或脊髓损伤患者重建运动控制能力；
				- **人工智能**：改进机器人或假肢的反应速度与适应性；
				- **人机交互**：设计更符合人类预期模式的智能辅助设备。
	- #model [Transformer要被取代了](https://mp.weixin.qq.com/s/D1H2U6O4w_JqPO6RCguZVQ) 很有意思的文章! 主要在说并行化不是万能的, 有些问题必须用到串行化处理, 并且在后续的模型设计中, 在空间维度使用Transformer, 而在时间维度使用LSTM.
	  collapsed:: true
		- 这篇论文探讨的是一个非常核心且有趣的问题：
			- ==**在处理超长序列任务时，纯粹的并行计算模型（如我们熟知的Transformer）是否足够？答案是否定的。**==
		- ### 一、核心问题：串行 vs. 并行
		  collapsed:: true
			- 想象一下你要完成两个任务：
				- **任务A（并行任务）**：计算一长串数字的总和。
					- 你可以把数字分成几组，让几个人同时计算各自组的总和，最后再把结果加起来。这非常高效。这就像是Transformer的“注意力机制”，可以同时看到所有的输入信息。
				- **任务B（串行任务）**：阅读一本侦探小说，并找出凶手。
					- 你**必须**从第一页开始，一页一页地读。因为每一章的信息都建立在上一章的基础上，凶手的信息可能分散在整本书的各个角落，你需要一步步积累线索，更新你对案件的“心理状态”。你无法跳着读，或者同时读所有页来直接得出结论。这个过程是** inherently serial**，即**天生串行**的。
			- **论文的核心观点就是**：当前最主流的大模型（如GPT系列使用的Transformer架构）更像任务A，它们擅长并行处理信息，但面对很多需要像任务B一样**长时间、一步步积累状态**的“智能体任务”时，会力不从心。
		- ### 二、关键概念与比喻
		  collapsed:: true
			- 为了说清楚这个问题，论文提出了几个关键概念：
			- **真实深度**
				- **比喻**：就是你读完那本侦探小说**必须**花费的最少时间。书的页数（序列长度）越多，你需要阅读的时间（真实深度）就越长。这是一个无法被压缩的、必须串行完成的过程。
				- **技术定义**：计算过程中必须顺序执行、无法并行的操作步骤数。
			- **循环完备性**
				- **比喻**：一个模型是否具备“读侦探小说”的能力。即，它能否用自己当前的“心理状态”（`h_t`），结合刚读到的新一页内容（`x_t`），来更新形成一个新的“心理状态”（`h_{t+1}`）。这个更新函数 `g` 可以是任何复杂的、非线性的过程。
				- **技术定义**：模型能否实现通用的、可能非线性的循环状态更新 `h_t = g(h_{t-1}, x_t)`。
			- **输入长度比例性**
				- **比喻**：侦探小说这个任务本身就是“输入长度比例”的——你想知道结局，就必须读完所有n页，需要O(n)的阅读步骤。
				- **技术定义**：一个问题要得到正确答案，所需要的真实串行步骤数与输入序列的长度成正比。
			- **输入聚合临界点**
				- **比喻**：一个不具备“读侦探小说”能力的模型（比如一个只能同时看所有页的模型），在小说篇幅较短时，或许还能连蒙带猜拼出个大概。但一旦小说长度超过某个**临界点**，它的理解就会完全崩溃，再也无法形成正确的“心理状态”。
				- **技术定义**：非循环完备模型在处理输入长度比例性问题时，所能正确聚合信息的最大序列长度。超过这个长度，模型性能会急剧下降。
		- ### 三、论文的“不可能三角”与诊断实验
		  collapsed:: true
			- 论文从理论上证明了一个“不可能三角”：
			  
			  > **一个模型无法同时拥有：1. 循环完备性；2. 可并行训练；3. 可并行推理。**
			  
			  像Transformer、Mamba等热门模型，为了追求训练和推理的效率（2和3），牺牲了**循环完备性（1）**。这意味着它们在理论上就无法完美解决所有需要长程串行计算的任务。
			- 为了验证这一点，作者设计了两个精巧的“诊断实验”：
				- **前向引用跳转任务**
					- **比喻**：这是一段特殊的“程序”，里面充满了“如果...就跳到第X步”的指令，而且只能向前跳转。
					- **为什么它难？** 要知道程序最终停在哪儿，你**必须**从头开始，一步步执行。因为你不知道下一步要执行哪条指令，直到你执行完当前这条。这完美模拟了必须串行计算的情景。
					- ==**实验结果**：Transformer和Mamba模型在这个任务上，当程序长度（深度）增加时，准确率会大幅下降。而一个简单的、串行计算的LSTM模型，却能在长得多的序列上保持很好的性能。==
				- **信息隐藏的迷宫位置追踪任务**
					- **基础版**：告诉你“左、左、上”，你只需要做个加法就知道最终位置。这很容易，所有模型都能做。
					- **困难版**：告诉你“向左移动”，但**不告诉你**这次移动是否撞墙了（位置未变）。要知道自己的真实位置，你必须根据迷宫地图，在心里一步步模拟移动过程。
					- **实验结果**：在困难版中，随着隐藏信息的步骤增多，Transformer的性能再次急剧恶化，而LSTM依然表现出色。
				- 这些实验强有力地证明了，在处理具有**输入长度比例性**的任务时，**循环完备性**是至关重要的。
		- ### 四、解决方案：循环完备的基于帧的动作模型
		  collapsed:: true
			- 既然问题找到了，怎么解决呢？论文提出了一个新的模型架构：
			- **数据来源**：
				- ==他们从一个意想不到的地方获取了海量的、带有动作序列的数据——**Git代码提交历史**。==通过技术手段，他们将每次代码提交还原成了用户在文本编辑器中的操作序列（按键、光标移动等），并把每个时刻的编辑器界面保存为一“帧”（就像终端屏幕截图）。这样就构成了“文本视频+操作动作”的庞大数据集。
			- **模型设计**：
				- 他们的模型由两部分组成：
					- **帧头**：一个Transformer，负责**并行地**理解**单个时间点**的屏幕画面（一帧），将其压缩成一个抽象的“向量”。
					- **主序列模型**：一个**LSTM堆栈**，负责**串行地**处理由帧头产生的一系列向量。LSTM就像一个有着内部记忆的“大脑”，一步步地观看“文本视频”，并基于之前所有的历史来预测用户的下一个操作。
			- **这个设计的精髓在于“扬长避短”**：
				- 用**Transformer**处理**空间信息**（一帧内的复杂内容），这是它擅长的。
				- 用**LSTM**处理**时间信息**（帧与帧之间的依赖关系），以获取**循环完备性**，解决长程依赖问题。
		- ### 五、惊人的发现：序列长度的“幂律缩放”
		  collapsed:: true
			- 在训练这个新模型时，他们发现了一个非常重要的规律：
				- > **在模型参数总量不变的情况下，仅仅增加训练时使用的序列长度（即让模型一次看更长的“文本视频”），模型的损失（可以理解为犯错率）会按照一个“幂律”下降。**
			- 简单来说，就是 **“看得越远，学得越好”**。
			- 更关键的是，虽然一次看更长的序列会让单次训练耗时变长（成本线性增加），但这个额外的成本是**值得的**。从总训练时间来看，长序列模型最终会“后来居上”，达到比短序列模型更低的错误率。这个优势一旦建立，就会一直保持。
		- ### 六、总结与启示
		  collapsed:: true
			- 这篇论文的核心思想可以总结为以下几点：
				- **挑战主流**：对“注意力机制即是一切”的观点提出挑战，指出纯粹的并行模型存在理论上的能力边界。
				- **理论奠基**：提出了“循环完备性”和“真实深度”等概念，并严格证明了并行模型无法解决所有问题。
				- **实证支持**：通过诊断实验，展示了现有模型在需要串行思考的任务上存在明显短板。
				- **提出方案**：设计了一种混合架构，结合了Transformer和LSTM的优点，以应对长程、序列化的智能体任务。
				- **发现规律**：发现了在固定参数下，模型性能随序列长度增长的“幂律缩放”现象，为未来模型的发展指出了一个可能的新方向——**向“长度”要性能**。
			- **对我们普通人的启示**：
				- 这项研究预示着，未来真正智能的、能像人类一样执行复杂多步任务（如全程编写一个软件、管理一个项目）的AI，可能不再是单一的“超级大脑”，而更像是一个 **“具备瞬时感知能力的Transformer眼睛 + 一个具备长久记忆和逻辑推理能力的LSTM大脑”** 的结合体。通往更强大AI的道路，或许不仅在于把模型做得更大，也在于让模型“想”得更久、更深入。
	- #world-model [李飞飞x李曼玲 | 押注的 “世界模型” 终有突破！3B VLM干翻GPT-5，揭开AI “看懂世界” 的密码](https://mp.weixin.qq.com/s/Z2np_LDZSAVdziCJvU-WYw) 感觉基本的思想和方法都好简单啊
	  collapsed:: true
		- 这篇论文提出了一个名为 **VAGEN** 的新方法，用来训练和提升一种特殊的AI模型——**视觉语言模型**——在复杂多步任务中的表现。
		- ### 🧠 核心思想：让AI学会“动脑筋”
		  collapsed:: true
			- 想象一下，你在一个光线很暗、视野受限的迷宫里找宝藏。
				- 你**看到**的（**Observation**）：只是手电筒照亮的一小片区域。
				- 你需要**知道**的（**State**）：是整个迷宫的地图、宝藏位置、你自己的位置。
			- 为了解决这个“看得不全”的问题，你的大脑会做两件事：
				- 1.  **状态估计**：根据看到的一角，在心里画一张“当前情况猜想图”。（“我面前是墙，左边好像有路，我听见右边有水滴声。”）
				  2.  **转移建模**：预测如果你采取某个行动，下一步会看到什么。（“如果我向左走，我可能会看到一个转角；如果我推这个箱子，它可能会挡住路。”）
			- 这个在心里“画图”和“推演”的过程，就是你的**世界模型**。
			- 这篇论文的核心就是：==**我们能不能教会视觉AI模型也建立这样的“世界模型”？**==
			- 论文的答案是：**能！** 而且通过一种巧妙的方法，可以让AI模型的这种“思考能力”大大增强。
		- ### 🧩 论文详解（通俗版）
		  collapsed:: true
			- #### 1. 问题：为什么普通的视觉AI“很笨”？
			  collapsed:: true
				- 我们现在有很多强大的视觉语言模型（比如GPT-4V，Gemini），它们能看懂图片并聊天。但是，让它们去完成一个需要**多个步骤**的**任务**时（比如在迷宫里导航、操作机械臂抓取物品），它们就表现得不太好。
				- **根本原因**：它们只是“看到什么就反应什么”，没有一个**持续的、内部的思维过程**来记住和推理环境的变化。它们像是在一个局部可见的世界里，走一步看一步，缺乏规划和预判能力。
			- #### 2. 解决方案：VAGEN框架——强制AI“写思考笔记”
			  collapsed:: true
				- VAGEN框架的核心是，在AI每次行动之前，**强制**它按照一个固定的格式输出两段“思考笔记”：
					- **`<observation>`（状态估计）**：描述“我现在看到了什么？”。
						- *例如（在推箱子游戏里）*：“我看到玩家在左下角，箱子在玩家的右边，目标点在玩家的下方。”
					- **`<prediction>`（转移建模）**：预测“我做完动作后，下一步会看到什么？”。
						- *例如*：“如果我向下移动，我会推到箱子，箱子会向下移动一格。”
					- 然后，才是真正的行动指令 `</answer>`。
				- **这个“写笔记”的过程，就是在逼着AI构建它的“世界模型”。**
			- #### 3. 如何训练？——“胡萝卜加大棒”的强化学习
			  collapsed:: true
				- 光有格式不行，还要通过训练让AI知道怎么“写笔记”才能得高分。这里用的是**强化学习**。
				- **大棒（惩罚）**：如果AI乱写、不按格式写，或者行动失败了，就扣分。
				- **胡萝卜（奖励）**：
					- **任务奖励**：最终完成任务，给大奖。
					- **格式奖励**：按要求写了“思考笔记”，给小奖。
					- **【VAGEN的创新奖励】世界建模奖励**：用一个更厉害的AI老师（LLM-as-a-Judge）来检查它的“思考笔记”写得对不对。如果AI对当前状态的描述和真实情况吻合，并且对下一步的预测也准确，就给它额外奖励。这让AI的“思考”不仅仅是敷衍，而是真正朝着准确理解环境的方向努力。
			- #### 4. 另一个大创新：更精细的“功劳分配”（Bi-Level GAE）
			  collapsed:: true
				- 在多步任务中，成功或失败是很多步行动共同作用的结果。怎么知道哪一步的“思考”是关键呢？
				- **传统方法**：像发年终奖，到最后才根据总业绩发一笔钱，很难说清每个员工的具体贡献。
				- **VAGEN的新方法（Bi-Level GAE）**：
					- 先评估**每一轮**（Turn-Level）的整体表现，给这一轮定一个“团队奖”。
					- 再把这个“团队奖”精细地分配给这一轮中**每一个生成的字**（Token-Level），看哪个字的贡献大。
					- 这样，AI就能更清楚地知道，是“思考笔记”里的哪个词用得好，导致了最后的成功，从而学得更快、更准。
		- ### 🏆 效果如何？
		  collapsed:: true
			- 论文在5个不同的任务上做了测试（推箱子、冰湖导航、3D室内导航、机器人操作、画矢量图）。结果非常惊人：
			- 一个仅有 **30亿参数** 的开源小模型（Qwen2.5-VL-3B），经过VAGEN训练后，**总体表现超过了GPT-5、Gemini 2.5 Pro、Claude 4.5** 等巨头公司的超大模型。
			- 相比于训练前，性能提升了将近 **3倍**。
			- 证明了**明确的视觉状态推理（写思考笔记）** 对于提升VLM在智能体任务中的表现至关重要。
		- ### 💎 总结
		  collapsed:: true
			- 你可以把VAGEN理解为一套**培养AI“特工”的训练体系**：
				- **教它方法**：要求它每次行动前，必须做“形势分析”（状态估计）和“沙盘推演”（转移建模）。
				- **派老师监督**：有专门的考官（LLM法官）检查它的分析报告写得是否准确。
				- **科学考评**：有一套精细的绩效考核制度（Bi-Level GAE），让它的每一步努力都能得到公正的评价和回报。
			- 最终，这套体系培养出的“AI特工”不再是只会蛮干的工具，而是变成了有思想、能规划、善预判的**智能体**。
			- 希望这个解释能帮助你完全理解这篇有趣且强大的工作！如果你对哪个部分还有疑问，我们可以继续讨论。